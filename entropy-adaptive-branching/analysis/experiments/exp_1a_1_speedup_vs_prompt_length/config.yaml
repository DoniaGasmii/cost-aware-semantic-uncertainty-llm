# Experiment 1.A.1: Speedup vs Prompt Length
# Research Question: Does EAB efficiency increase with prompt length?
# Hypothesis: Linear or super-linear speedup growth (longer prompts = more shared computation)

experiment:
  name: "exp_1a_1_speedup_vs_prompt_length"
  description: "Measure EAB speedup across different prompt lengths (FIXED sample count)"
  date_created: "2025-01-01"
  parent_experiment: "1.A - Efficiency Analysis"
  sub_experiment: "1 of 4"

# Model configuration (FIXED)
model:
  name: "gpt2"  # Start with GPT-2 (124M) for fast debugging
  device: "cpu"  # Use "cuda" if GPU available
  dtype: "float32"

# Generation parameters (FIXED)
generation:
  max_new_tokens: 50
  temperature: 0.8
  top_p: 1.0
  do_sample: true

# EAB parameters (FIXED)
eab:
  entropy_threshold: 0.4
  branch_factor: 3
  max_paths: 20

# Naive sampling parameters
naive:
  temperature: 0.8
  top_p: 1.0

# CRITICAL: Fixed sample count for fair comparison
# Both EAB and Naive will generate EXACTLY this many samples
target_samples: 20  # FIXED across all prompts

# Experimental variables (INDEPENDENT VARIABLE)
prompt_lengths: [50, 100, 200, 500]  # in tokens
prompts_per_length: 10  # number of prompts to test per length

# Debug mode (for quick testing)
debug:
  enabled: false  # Set to true for quick testing
  prompt_lengths: [50, 200]  # Only 2 lengths
  prompts_per_length: 2  # Only 2 prompts per length
  target_samples: 10  # Fewer samples for quick testing

# Metrics to track
metrics:
  cost:
    - "token_steps"        # Total forward passes Ã— tokens (FLOPs proxy)
    - "wall_clock_time"    # Real execution time (seconds)
    - "memory_peak_mb"     # Peak memory usage (MB)
    - "tokens_per_sample"  # Total tokens / number of samples

  efficiency:
    - "speedup_factor"     # Naive cost / EAB cost
    - "cost_ratio"         # EAB cost / Naive cost

  branching:
    - "branch_count"       # Total number of branches
    - "branch_frequency"   # Branches per token position
    - "avg_branch_position"  # Average token position where branching occurs
    - "final_path_count"   # Number of active paths at end

# Output configuration
output:
  save_raw_results: true
  save_intermediate: true  # Save results after each prompt
  results_dir: "results"
  figures_dir: "results/figures"

# Reproducibility
seed: 42
