# Experiment 1.A.1: Speedup vs Prompt Length
# Research Question: Does EAB efficiency increase with prompt length?
# Hypothesis: Linear or super-linear speedup growth (longer prompts = more shared computation)

experiment:
  name: "exp_1a_1_speedup_vs_prompt_length"
  description: "Measure EAB speedup across different prompt lengths (natural EAB behavior)"
  date_created: "2025-01-01"
  parent_experiment: "1.A - Efficiency Analysis"
  sub_experiment: "1 of 4"

# Model configuration
model:
  name: "Qwen/Qwen2.5-3B-Instruct"
  device: "cuda"
  dtype: "float16"

# Generation parameters
generation:
  max_new_tokens: 30
  temperature: 0.8
  top_p: 1.0
  do_sample: true

# EAB parameters
eab:
  entropy_threshold: 0.055
  branch_factor: 3
  max_paths: 20

# Naive sampling parameters (will match EAB sample count)
naive:
  temperature: 0.8
  top_p: 1.0

# Fair Comparison Protocol (from README):
# 1. Run EAB with natural behavior → generates N samples (varies by prompt)
# 2. Run Naive N times → match EAB's sample count
# 3. Compare costs fairly
#
# Note: target_samples below is just a reference, NOT enforced
target_samples: 20  # Reference only - actual count determined by EAB

# Experimental variables (INDEPENDENT VARIABLE)
prompt_lengths: [50, 100, 200, 500]  # in tokens
prompts_per_length: 10  # number of prompts to test per length

# Debug mode (for quick testing)
debug:
  enabled: true  # Set to true for quick testing
  prompt_lengths: [50, 200]  # Only 2 lengths
  prompts_per_length: 2  # Only 2 prompts per length
  target_samples: 10  # Reference only (actual count from EAB)

# Metrics to track
metrics:
  cost:
    - "token_steps"        # Total forward passes × tokens (FLOPs proxy)
    - "wall_clock_time"    # Real execution time (seconds)
    - "memory_peak_mb"     # Peak memory usage (MB)
    - "tokens_per_sample"  # Total tokens / number of samples

  efficiency:
    - "speedup_factor"     # Naive cost / EAB cost
    - "cost_ratio"         # EAB cost / Naive cost

  branching:
    - "branch_count"       # Total number of branches
    - "branch_frequency"   # Branches per token position
    - "avg_branch_position"  # Average token position where branching occurs
    - "final_path_count"   # Number of active paths at end

# Output configuration
output:
  save_raw_results: true
  save_intermediate: true  # Save results after each prompt
  save_generated_texts: true  # Save actual generated responses for inspection
  results_dir: "results"
  figures_dir: "results/figures"
  texts_dir: "results/generated_texts"  # Human-readable outputs

# Reproducibility
seed: 42
