{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy-Adaptive Branching Tutorial\n",
    "\n",
    "This notebook provides an interactive tutorial for using Entropy-Adaptive Branching (EAB) for efficient multi-sample generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if not already installed\n",
    "# !pip install -e ..\n",
    "\n",
    "from eab import EntropyAdaptiveBranching\n",
    "from eab.utils import set_seed, visualize_branching_statistics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize EAB\n",
    "\n",
    "We'll start with a small model (GPT-2) for quick experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eab = EntropyAdaptiveBranching(\n",
    "    model_name=\"gpt2\",\n",
    "    entropy_threshold=0.4,\n",
    "    branch_factor=3,\n",
    "    max_paths=20\n",
    ")\n",
    "\n",
    "print(\"✓ EAB initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Generation\n",
    "\n",
    "Let's generate multiple completions for a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "\n",
    "results = eab.generate(\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=10,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(results)} completions:\\n\")\n",
    "for i, result in enumerate(results[:5], 1):\n",
    "    print(f\"{i}. {result['text']} (p={result['probability']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing Entropy\n",
    "\n",
    "Let's examine how entropy evolved during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entropy history\n",
    "entropy_history = eab.get_entropy_history()\n",
    "\n",
    "print(\"Entropy Statistics:\")\n",
    "stats = entropy_history['statistics']\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Plot entropy evolution\n",
    "eab.plot_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Different Prompts\n",
    "\n",
    "Let's compare branching behavior for different types of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    (\"The capital of Japan is\", \"factual\"),\n",
    "    (\"In my opinion, the best movie is\", \"subjective\"),\n",
    "    (\"Once upon a time,\", \"creative\")\n",
    "]\n",
    "\n",
    "for prompt, prompt_type in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt ({prompt_type}): {prompt}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    results = eab.generate(\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=15,\n",
    "        temperature=0.9,\n",
    "        show_progress=False\n",
    "    )\n",
    "    \n",
    "    stats = eab.get_entropy_history()['statistics']\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Paths: {len(results)}\")\n",
    "    print(f\"  Branch rate: {stats['branch_rate']:.1%}\")\n",
    "    print(f\"  Avg entropy: {stats['mean_entropy']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nTop 3 completions:\")\n",
    "    for i, r in enumerate(results[:3], 1):\n",
    "        print(f\"  {i}. {r['text'][:60]}... (p={r['probability']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tuning Hyperparameters\n",
    "\n",
    "Let's see how different entropy thresholds affect generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The best way to learn programming is\"\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    eab.set_entropy_threshold(threshold)\n",
    "    \n",
    "    results = eab.generate(\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=20,\n",
    "        temperature=0.9,\n",
    "        show_progress=False\n",
    "    )\n",
    "    \n",
    "    stats = eab.get_entropy_history()['statistics']\n",
    "    \n",
    "    print(f\"\\nThreshold={threshold}:\")\n",
    "    print(f\"  Paths: {len(results)}\")\n",
    "    print(f\"  Branches: {stats['num_branches']}\")\n",
    "    print(f\"  Branch rate: {stats['branch_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Branching Statistics\n",
    "\n",
    "Let's create comprehensive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with moderate settings\n",
    "eab.set_entropy_threshold(0.4)\n",
    "eab.set_branch_factor(3)\n",
    "\n",
    "results = eab.generate(\n",
    "    prompt=\"The future of artificial intelligence will be\",\n",
    "    max_new_tokens=30,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "# Convert results to path objects for visualization\n",
    "from eab.path import GenerationPath\n",
    "paths = []\n",
    "for r in results:\n",
    "    path = GenerationPath(\n",
    "        tokens=r['tokens'],\n",
    "        log_prob=r['log_prob'],\n",
    "        branch_points=r.get('branch_points', []),\n",
    "        path_id=r.get('path_id')\n",
    "    )\n",
    "    paths.append(path)\n",
    "\n",
    "# Visualize\n",
    "visualize_branching_statistics(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Uncertainty Quantification\n",
    "\n",
    "Let's use EAB for semantic uncertainty analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Generate samples\n",
    "results = eab.generate(\n",
    "    prompt=\"The most important quality in a leader is\",\n",
    "    max_new_tokens=15,\n",
    "    temperature=0.9\n",
    ")\n",
    "\n",
    "# Extract first word of each completion\n",
    "first_words = [r['text'].strip().split()[0] for r in results if r['text'].strip()]\n",
    "word_counts = Counter(first_words)\n",
    "\n",
    "print(f\"\\nFirst word distribution:\")\n",
    "for word, count in word_counts.most_common(5):\n",
    "    print(f\"  '{word}': {count} times ({count/len(first_words):.1%})\")\n",
    "\n",
    "# Compute uncertainty\n",
    "import numpy as np\n",
    "probs = np.array([count for count in word_counts.values()]) / len(first_words)\n",
    "entropy = -np.sum(probs * np.log(probs))\n",
    "\n",
    "print(f\"\\nSemantic uncertainty (entropy): {entropy:.3f}\")\n",
    "print(f\"Interpretation: {'High' if entropy > 1.5 else 'Medium' if entropy > 1.0 else 'Low'} uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. **Factual prompts** → Low branching, low entropy\n",
    "2. **Ambiguous prompts** → Medium branching, medium entropy  \n",
    "3. **Creative prompts** → High branching, high entropy\n",
    "\n",
    "4. **Lower threshold** → More aggressive branching\n",
    "5. **Higher threshold** → More conservative branching\n",
    "\n",
    "6. **Efficiency**: Shared computation for all tokens until branching\n",
    "7. **Diversity**: Multiple samples from single generation pass\n",
    "8. **Uncertainty**: Branching patterns reveal model confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Try different models (GPT-2 Medium, Large)\n",
    "- Experiment with different hyperparameters\n",
    "- Apply to your own use cases\n",
    "- Integrate with uncertainty quantification pipelines\n",
    "- See `examples/` directory for more advanced usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
